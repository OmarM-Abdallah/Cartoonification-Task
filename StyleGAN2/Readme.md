
# StyleGAN2

The style-based GAN architecture [(StyleGAN)](https://drive.google.com/file/d/1fnF-QsiQeKaxF-HbvFiGtzHF_Bf3CzJu/view) yields state-of-the-art results in data-driven unconditional generative image modeling. this improved model redefines the state of the art in unconditional image modeling, both in terms of existing distribution quality metrics as well as perceived image quality.


### Contents
- Introduction
- How It Works
- Installation Requirements
- Usage Guide
### Introduction
The current state-of-the-art method for high-resolution image synthesis is StyleGAN, which has been shown to work reliably on a variety of datasets. 

StyleGAN2 focuses on fixing its characteristic artifacts and improving the result quality further. Many observers have noticed characteristic artifacts in images generated by StyleGAN, StyleGAN2 identifies two causes for these artifacts, and describe changes in architecture and training methods that eliminate them. 

**1-**  the origin of common blob-like artifacts is investigated , and it was found that the generator creates them to circumvent a design flaw in its architecture.
so the normalization used in the generator was redesigned, which removes the artifacts. 

**2-** the artifacts related to progressive growing that has been highly successful in stabilizing high-resolution GAN training. the Paper proposes an alternative design that achieves the same goal (training starts by focusing on low-resolution images and then progressively shifts focus to higher and higher resolutions) without changing the network topology
during training. This new design also maes it possible to reason
about the effective resolution of the generated images, which turns out to be lower than expected, motivating a capacity increase.

![](https://github.com/OmarM-Abdallah/Cartoonification-Task/blob/main/StyleGAN2/Untitled.png)
**Figure 1.** Instance normalization causes water droplet -like artifacts in StyleGAN images.


### How It Works
the work done here to solve our problem is the work of a specific model that is based off StyleGAN2 called Toonify, it simply uses the general architecture of StyleGAN2 to further achieve our goal to toonify images, you can find the original Toonify repo [here.](https://github.com/justinpinkney/toonify) 

next we will discuss the general workflow to use Toonify to cartoonize our images.

- **Upload your own photos**

Upload your photos to `raw/`. These don't need to be aligned as we'll use a face detector to grab all the faces and transform them into the correct format. One note of caution is that you'll need a pretty high-resolution picture of a face to get a sharp result (the final face crop is resized to 1024x1024 pixels)

The basic process is:
- Extract faces and align the images.
- Project the images (i.e. find the latent code).
- Toonify the images (i.e. use the latent code with the toon model).

Results will be placed in the stylegan2/generated folder.


### Installation Requirements

- Both Linux and Windows are supported. Linux is recommended for performance and compatibility reasons.
- 64-bit Python 3.6, Anaconda3 with numpy >= 1.14.3.
- TensorFlow 1.x (only TF 1.14 for windows).
- CUDA 10.0 toolkit and cuDNN 7.5.

**Note:** StyleGAN2 relies on custom TensorFlow ops that are compiled on the fly using [NVCC](https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html). To test that your NVCC installation is working correctly, run:

```.bash
nvcc test_nvcc.cu -o test_nvcc -run
| CPU says hello.
| GPU says hello.
```
### Usage Guide

**1.** after moving the test images to the `raw/` folder, run the following code block to import the pretrained model and run testing with it.

```python
import pretrained_networks

# use my copy of the blended model to save Doron's download bandwidth
# get the original here https://mega.nz/folder/OtllzJwa#C947mCCdEfMCRTWnDcs4qw
blended_url = "https://drive.google.com/uc?id=1H73TfV5gQ9ot7slSed_l-lim9X7pMRiU" 
ffhq_url = "http://d36zk2xti64re0.cloudfront.net/stylegan2/networks/stylegan2-ffhq-config-f.pkl"

_, _, Gs_blended = pretrained_networks.load_networks(blended_url)
_, _, Gs = pretrained_networks.load_networks(ffhq_url)
```

##### 2. aligning images
this step is done to Extract and align all faces from images using DLib and a function from original FFHQ dataset preparation step.

`raw/` is the dir containing our images.

`aligned/` is the dir where the aligned images will be saved to.

>`python3 align_images.py raw/ aligned/`

##### 3. Project images
this step is used to Find latent representation of aligned images.

`--num-steps` argument define the Number of optimization steps.

`generated/` is the dir will the result projected images will be saved to.


>`python3 project_images.py --num-steps 500 aligned/ generated/`

##### 4. Toonify the images

this step is used to apply the last step, which is to apply Network blending. in StyleGAN Swapping layers between two models in StyleGAN gives some interesting results. You need a base model and a second model which has been fine-tuned from the base.
 
my understanding is that this step helps to apply more cartoonized effect to result images, as we apply intermediate features to them that help apply the style of cartoonization more.

```python
import numpy as np
from PIL import Image
import dnnlib
import dnnlib.tflib as tflib
from pathlib import Path

latent_dir = Path("generated")
latents = latent_dir.glob("*.npy")
for latent_file in latents:
  latent = np.load(latent_file)
  latent = np.expand_dims(latent,axis=0)
  synthesis_kwargs = dict(output_transform=dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=False), minibatch_size=8)
  images = Gs_blended.components.synthesis.run(latent, randomize_noise=False, **synthesis_kwargs)
  Image.fromarray(images.transpose((0,2,3,1))[0], 'RGB').save(latent_file.parent / (f"{latent_file.stem}-toon.jpg"))
print("inferance time= ",time.time()-start) 
```
